# SilentAid

An AI accessibility tool that transcribes speech to text and adds emotional context so users can see not just what is said, but how itâ€™s said.

## Overview

SilentAid runs in the browser for live speech recognition and uses a Python/Flask backend with a HuggingFace model to classify emotions for each final transcript chunk. Lines are rendered with an emoji + emotion tag and color-coded for quick readability.

## Features

- Live transcription (Web Speech API)
- Emotion detection per line (Happy, Sad, Angry, Excited, Neutral)
- Emoji + color tag prefix (e.g., ğŸ˜  [Angry] textâ€¦)
- Status indicator: Idle / Listening / Processing / Stopped
- A+ / A- font size controls (persisted)
- Transcript history: last 20 visible, last 50 saved locally
- Clear error feedback and accessible UI

## Tech Stack

- Frontend: HTML, CSS, JavaScript (ES6), Web Speech API, LocalStorage
- Backend: Python, Flask, Flask-CORS
- AI/ML: HuggingFace Transformers (DistilRoBERTa), PyTorch
- Model: j-hartmann/emotion-english-distilroberta-base (text-based emotion)

## Architecture

- Frontend (Browser)
  - Captures microphone audio with Web Speech API
  - Sends final transcript text to backend
  - Receives {text, emotion, confidence} and renders the line
  - Stores and restores history from localStorage
- Backend (Flask)
  - /api/health â€” health check, model status
  - /api/emotion â€” POST { text } â†’ returns { text, emotion, emoji/color mapping, confidence }
  - Uses Transformers pipeline to classify emotions; adds heuristics to better separate Excited from Happy

## Screenshots

Add images (PNG/JPG) into a `docs/` folder and update paths below.

- App Header & Controls
  - ![SilentAid Header](docs/header.png)
- Live Transcript with Emotions
  - ![SilentAid Transcript](docs/transcript.png)
- Font Controls & History
  - ![SilentAid Controls](docs/controls.png)

Tip: On macOS you can take screenshots with Shift+Cmd+4 and save into `docs/`.

## Quick Start

1. Start the backend (first run downloads the model):

- macOS/Linux:
  - `./start_backend.sh`
- Windows (PowerShell):
  - `python -m venv backend\venv`
  - `backend\venv\Scripts\activate`
  - `pip install -r backend\requirements.txt`
  - `python backend\app.py`

2. Open the frontend in Chrome:

- Open `frontend/index.html`

## How to Use (Demo Guide)

1. Click â€œStart Listeningâ€ and allow microphone access.
2. Speak short sentences; each final chunk is analyzed.
3. See lines appear with an emoji + color-coded emotion tag.
4. Use A+ / A- to adjust font size.
5. Click â€œStopâ€ to stop recognition (status shows Stopped).

Emotion cues examples:

- Excited: â€œWow! This is amazing! I canâ€™t wait!â€
- Happy: â€œIâ€™m really pleased with the result.â€
- Sad: â€œIâ€™m disappointed about what happened.â€
- Angry: â€œThis is unacceptable and frustrating.â€

## Color Mapping

- ğŸ˜Š Happy â€” Green
- ğŸ˜¢ Sad â€” Blue
- ğŸ˜  Angry â€” Red
- ğŸ¤© Excited â€” Orange
- ğŸ˜ Neutral â€” Gray

## API

- GET /api/health â†’ `{ status, model_loaded }`
- POST /api/emotion â†’ body `{ text: string }` returns:

```json
{
  "text": "I canâ€™t do this right now.",
  "emotion": "Angry",
  "emoji": "ğŸ˜ ",
  "color": "#ef4444",
  "confidence": 0.91,
  "timestamp": "2025-08-31T12:34:56.000Z"
}
```

## Project Structure

```
SilentAid/
â”œâ”€ frontend/
â”‚  â”œâ”€ index.html
â”‚  â”œâ”€ style.css
â”‚  â””â”€ script.js
â”œâ”€ backend/
â”‚  â”œâ”€ app.py
â”‚  â””â”€ requirements.txt
â”œâ”€ start_backend.sh
â””â”€ README.md
```

## Notes & Limitations

- Best in Chrome (Web Speech API support)
- English text only (MVP)
- Emotion is inferred from text (not vocal tone)
- First backend run needs Internet to download the model

## Credits

Built with Web Speech API, Flask, and HuggingFace Transformers for the hackathon demo of SilentAid â€” accessibility-focused, fast to run, and easy to show.
